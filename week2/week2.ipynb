{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc96HGV0pYkJ"
      },
      "source": [
        "# Introduction to Data Science 2025\n",
        "\n",
        "# Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWh-qfRJpYkK"
      },
      "source": [
        "## Exercise 1 | Titanic: data preprocessing and imputation\n",
        "<span style=\"font-weight: bold\"> *Note: You can find tutorials for NumPy and Pandas under 'Useful tutorials' in the course material.*</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZy31vJxpYkK"
      },
      "source": [
        "Download the [Titanic dataset](https://www.kaggle.com/c/titanic) [train.csv] from Kaggle or <span style=\"font-weight: 500\">directly from the course material</span>, and complete the following exercises. If you choose to download the dataset from Kaggle, you will need to create a Kaggle account unless you already have one, but it is quite straightforward.\n",
        "\n",
        "The dataset consists of personal information of all the passengers on board the RMS Titanic, along with information about whether they survived the iceberg collision or not.\n",
        "\n",
        "1. Your first task is to read the data file and print the shape of the data.\n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint 1: You can read them into a Pandas dataframe if you wish.*</span>\n",
        "    \n",
        "    <span style=\"font-weight: 500\"> *Hint 2: The shape of the data should be (891, 12).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "l4kCNDespYkK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(891, 12)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "shape = data.shape\n",
        "print(shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCZoOS-wpYkL"
      },
      "source": [
        "2. Let's look at the data and get started with some preprocessing. Some of the columns, e.g <span style=\"font-weight: 500\"> *Name*</span>, simply identify a person and are not useful for prediction tasks. Try to identify these columns, and remove them.\n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint: The shape of the data should now be (891, 9).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "m18V-jpTpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(891, 9)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train.csv')\n",
        "df = df.drop(['Name'], axis=1)\n",
        "df = df.drop(['Fare'], axis=1)\n",
        "df = df.drop(['Ticket'], axis=1)\n",
        "shape = df.shape\n",
        "print(shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48AcPDOrpYkL"
      },
      "source": [
        "3. The column <span style=\"font-weight: 500\">*Cabin*</span> contains a letter and a number. A smart catch at this point would be to notice that the letter stands for the deck level on the ship. Keeping just the deck information would be more informative when developing, e.g. a classifier that predicts whether a passenger survived. The next step in our preprocessing will be to add a new column to the dataset, which consists simply of the deck letter. You can then remove the original <span style=\"font-weight: 500\">*Cabin*</span>-column.\n",
        "\n",
        "<span style=\"font-weight: 500\">*Hint: The deck letters should be ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'T'].*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OQX5LmWrpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(183, 9)\n",
            "Cabin_let\n",
            "        C\n",
            "        C\n",
            "        E\n",
            "        G\n",
            "        C\n",
            "        D\n",
            "        A\n",
            "    C C C\n",
            "        D\n",
            "        B\n",
            "        C\n",
            "        F\n",
            "      F G\n",
            "    C C C\n",
            "        E\n",
            "        A\n",
            "      D D\n",
            "        D\n",
            "        C\n",
            "      B B\n",
            "        E\n",
            "        D\n",
            "        D\n",
            "        C\n",
            "        B\n",
            "        F\n",
            "        C\n",
            "        B\n",
            "        A\n",
            "        C\n",
            "        F\n",
            "        F\n",
            "        B\n",
            "        B\n",
            "        G\n",
            "        A\n",
            "        D\n",
            "        D\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        D\n",
            "        G\n",
            "        C\n",
            "        B\n",
            "        E\n",
            "        B\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        D\n",
            "        B\n",
            "        D\n",
            "      C C\n",
            "      B B\n",
            "      C C\n",
            "        C\n",
            "        E\n",
            "        C\n",
            "  B B B B\n",
            "        C\n",
            "        E\n",
            "        C\n",
            "        D\n",
            "        B\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        E\n",
            "        T\n",
            "        F\n",
            "    C C C\n",
            "        F\n",
            "        E\n",
            "        D\n",
            "        B\n",
            "        E\n",
            "        C\n",
            "      B B\n",
            "        D\n",
            "        G\n",
            "        C\n",
            "        E\n",
            "        C\n",
            "        E\n",
            "      B B\n",
            "    C C C\n",
            "        A\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        E\n",
            "        E\n",
            "        E\n",
            "        D\n",
            "        B\n",
            "        C\n",
            "        B\n",
            "        C\n",
            "        D\n",
            "      C C\n",
            "        B\n",
            "        C\n",
            "        E\n",
            "        D\n",
            "        F\n",
            "        B\n",
            "        B\n",
            "        B\n",
            "        B\n",
            "        B\n",
            "        C\n",
            "        C\n",
            "        A\n",
            "        E\n",
            "        C\n",
            "        E\n",
            "        E\n",
            "        C\n",
            "        A\n",
            "        E\n",
            "        B\n",
            "        D\n",
            "        A\n",
            "        C\n",
            "        F\n",
            "        D\n",
            "        D\n",
            "        D\n",
            "        A\n",
            "        B\n",
            "        B\n",
            "        D\n",
            "        A\n",
            "        D\n",
            "        E\n",
            "        B\n",
            "    B B B\n",
            "        D\n",
            "        B\n",
            "        B\n",
            "        C\n",
            "      F G\n",
            "      C C\n",
            "        E\n",
            "        E\n",
            "        C\n",
            "        C\n",
            "      F G\n",
            "        C\n",
            "        E\n",
            "        E\n",
            "        B\n",
            "        B\n",
            "        C\n",
            "  B B B B\n",
            "        B\n",
            "        D\n",
            "        E\n",
            "        B\n",
            "      B B\n",
            "        D\n",
            "        E\n",
            "        B\n",
            "        B\n",
            "        D\n",
            "      B B\n",
            "        D\n",
            "      B B\n",
            "        A\n",
            "        E\n",
            "        B\n",
            "        E\n",
            "        E\n",
            "        D\n",
            "        E\n",
            "        D\n",
            "        A\n",
            "        D\n",
            "    B B B\n",
            "        C\n",
            "        B\n",
            "        C\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train.csv')\n",
        "df = df.drop(['Name'], axis=1)\n",
        "df = df.drop(['Fare'], axis=1)\n",
        "df = df.drop(['Ticket'], axis=1)\n",
        "df['Cabin_let'] = df['Cabin']\n",
        "df['Cabin_let'] = df['Cabin_let'].str.replace('\\d+', '', regex=True)\n",
        "df = df.drop(['Cabin'], axis=1)\n",
        "df = df.dropna()\n",
        "shape = df.shape\n",
        "print(shape)\n",
        "print(df[['Cabin_let']].to_string(index=False)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupSH5tXpYkL"
      },
      "source": [
        "4. You’ll notice that some of the columns, such as the previously added deck number, are [categorical](https://en.wikipedia.org/wiki/Categorical_variable). To preprocess the categorical variables so that they're ready for further computation, we need to avoid the current string format of the values. This means the next step for each categorical variable is to transform the string values to numeric ones, that correspond to a unique integer ID representative of each distinct category. This process is called label encoding and you can read more about it [here](https://pandas.pydata.org/docs/user_guide/categorical.html).\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint: Pandas can do this for you.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "cfb0_uzRpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cabin_let\n",
            "        C\n",
            "        C\n",
            "        E\n",
            "        G\n",
            "        C\n",
            "        D\n",
            "        A\n",
            "    C C C\n",
            "        D\n",
            "        B\n",
            "        C\n",
            "        F\n",
            "      F G\n",
            "    C C C\n",
            "        E\n",
            "        A\n",
            "      D D\n",
            "        D\n",
            "        C\n",
            "      B B\n",
            "        E\n",
            "        D\n",
            "        D\n",
            "        C\n",
            "        B\n",
            "        F\n",
            "        C\n",
            "        B\n",
            "        A\n",
            "        C\n",
            "        F\n",
            "        F\n",
            "        B\n",
            "        B\n",
            "        G\n",
            "        A\n",
            "        D\n",
            "        D\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        D\n",
            "        G\n",
            "        C\n",
            "        B\n",
            "        E\n",
            "        B\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        D\n",
            "        B\n",
            "        D\n",
            "      C C\n",
            "      B B\n",
            "      C C\n",
            "        C\n",
            "        E\n",
            "        C\n",
            "  B B B B\n",
            "        C\n",
            "        E\n",
            "        C\n",
            "        D\n",
            "        B\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        E\n",
            "        T\n",
            "        F\n",
            "    C C C\n",
            "        F\n",
            "        E\n",
            "        D\n",
            "        B\n",
            "        E\n",
            "        C\n",
            "      B B\n",
            "        D\n",
            "        G\n",
            "        C\n",
            "        E\n",
            "        C\n",
            "        E\n",
            "      B B\n",
            "    C C C\n",
            "        A\n",
            "        C\n",
            "        C\n",
            "        C\n",
            "        E\n",
            "        E\n",
            "        E\n",
            "        D\n",
            "        B\n",
            "        C\n",
            "        B\n",
            "        C\n",
            "        D\n",
            "      C C\n",
            "        B\n",
            "        C\n",
            "        E\n",
            "        D\n",
            "        F\n",
            "        B\n",
            "        B\n",
            "        B\n",
            "        B\n",
            "        B\n",
            "        C\n",
            "        C\n",
            "        A\n",
            "        E\n",
            "        C\n",
            "        E\n",
            "        E\n",
            "        C\n",
            "        A\n",
            "        E\n",
            "        B\n",
            "        D\n",
            "        A\n",
            "        C\n",
            "        F\n",
            "        D\n",
            "        D\n",
            "        D\n",
            "        A\n",
            "        B\n",
            "        B\n",
            "        D\n",
            "        A\n",
            "        D\n",
            "        E\n",
            "        B\n",
            "    B B B\n",
            "        D\n",
            "        B\n",
            "        B\n",
            "        C\n",
            "      F G\n",
            "      C C\n",
            "        E\n",
            "        E\n",
            "        C\n",
            "        C\n",
            "      F G\n",
            "        C\n",
            "        E\n",
            "        E\n",
            "        B\n",
            "        B\n",
            "        C\n",
            "  B B B B\n",
            "        B\n",
            "        D\n",
            "        E\n",
            "        B\n",
            "      B B\n",
            "        D\n",
            "        E\n",
            "        B\n",
            "        B\n",
            "        D\n",
            "      B B\n",
            "        D\n",
            "      B B\n",
            "        A\n",
            "        E\n",
            "        B\n",
            "        E\n",
            "        E\n",
            "        D\n",
            "        E\n",
            "        D\n",
            "        A\n",
            "        D\n",
            "    B B B\n",
            "        C\n",
            "        B\n",
            "        C\n",
            "    PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch Embarked  Cabin_let\n",
            "1             2         1       1    0  38.0      1      0        C          5\n",
            "3             4         1       1    0  35.0      1      0        S          5\n",
            "6             7         0       1    1  54.0      0      0        S         10\n",
            "10           11         1       3    0   4.0      1      1        S         13\n",
            "11           12         1       1    0  58.0      0      0        S          5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train.csv')\n",
        "df = df.drop(['Name', 'Fare', 'Ticket'], axis=1)\n",
        "\n",
        "df['Cabin_let'] = df['Cabin']\n",
        "df['Cabin_let'] = df['Cabin_let'].str.replace('\\d+', '', regex=True)\n",
        "df = df.drop(['Cabin'], axis=1)\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "print(df[['Cabin_let']].to_string(index=False))\n",
        "\n",
        "df[\"Sex\"] = df[\"Sex\"].astype(\"category\").cat.codes\n",
        "df[\"Cabin_let\"] = df[\"Cabin_let\"].astype(\"category\").cat.codes\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32_hHxGLpYkL"
      },
      "source": [
        "5. Next, let's look into missing value **imputation**. Some of the rows in the data have missing values, e.g when the cabin number of a person is unknown. Most machine learning algorithms have trouble with missing values, and they need to be handled during preprocessing:\n",
        "\n",
        "    a) For continuous variables, replace the missing values with the mean of the non-missing values of that column.\n",
        "\n",
        "    b) For categorical variables, replace the missing values with the mode of the column.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Remember: Even though in the previous step we transformed categorical variables into their numeric representation, they are still categorical.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "e0kh2bbGpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch  Embarked  Cabin_let\n",
            "0            1         0       3    1  22.0      1      0         2          5\n",
            "1            2         1       1    0  38.0      1      0         0          5\n",
            "2            3         1       3    0  26.0      0      0         2          5\n",
            "3            4         1       1    0  35.0      1      0         2          5\n",
            "4            5         0       3    1  35.0      0      0         2          5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train.csv')\n",
        "df = df.drop(['Name', 'Fare', 'Ticket'], axis=1)\n",
        "\n",
        "df['Cabin_let'] = df['Cabin']\n",
        "df['Cabin_let'] = df['Cabin_let'].str.replace('\\d+', '', regex=True)\n",
        "df = df.drop(['Cabin'], axis=1)\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['float64', 'int64']:\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "    else:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "df[\"Sex\"] = df[\"Sex\"].astype(\"category\").cat.codes\n",
        "df[\"Cabin_let\"] = df[\"Cabin_let\"].astype(\"category\").cat.codes\n",
        "embarked_map = {\"C\": 0, \"Q\": 1, \"S\": 2}\n",
        "df[\"Embarked\"] = df[\"Embarked\"].map(embarked_map)\n",
        "\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Kbmgx9pYkM"
      },
      "source": [
        "6. At this point, all data is numeric. Write the data, with the modifications we made, to a  <span style=\"font-weight: 500\"> .csv</span> file. Then, write another file, this time in <span style=\"font-weight: 500\">JSON</span> format, with the following structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "J_EhC78NpYkM"
      },
      "outputs": [],
      "source": [
        "#[\n",
        "#    {\n",
        "#        \"Deck\": 0,\n",
        "#        \"Age\": 20,\n",
        "#        \"Survived\", 0\n",
        "#        ...\n",
        "#    },\n",
        "#    {\n",
        "#        ...\n",
        "#    }\n",
        "#]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "lxF-ehbapYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch  Embarked  Cabin_let\n",
            "0            1         0       3    1  22.0      1      0         2          5\n",
            "1            2         1       1    0  38.0      1      0         0          5\n",
            "2            3         1       3    0  26.0      0      0         2          5\n",
            "3            4         1       1    0  35.0      1      0         2          5\n",
            "4            5         0       3    1  35.0      0      0         2          5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train.csv')\n",
        "df = df.drop(['Name', 'Fare', 'Ticket'], axis=1)\n",
        "\n",
        "df['Cabin_let'] = df['Cabin']\n",
        "df['Cabin_let'] = df['Cabin_let'].str.replace('\\d+', '', regex=True)\n",
        "df = df.drop(['Cabin'], axis=1)\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['float64', 'int64']:\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "    else:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "df[\"Sex\"] = df[\"Sex\"].astype(\"category\").cat.codes\n",
        "df[\"Cabin_let\"] = df[\"Cabin_let\"].astype(\"category\").cat.codes\n",
        "embarked_map = {\"C\": 0, \"Q\": 1, \"S\": 2}\n",
        "df[\"Embarked\"] = df[\"Embarked\"].map(embarked_map)\n",
        "\n",
        "\n",
        "print(df.head())\n",
        "df.to_csv('train2.csv', index=False)\n",
        "json_data = df.to_dict(orient=\"records\")\n",
        "import json\n",
        "with open(\"train2.json\", \"w\") as f:\n",
        "    json.dump(json_data, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnkAgzHjpYkM"
      },
      "source": [
        "Study the records and try to see if there is any evident pattern in terms of chances of survival."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddqs0UqLpYkM"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGnzPePKpYkM"
      },
      "source": [
        "## Exercise 2 | Titanic 2.0: exploratory data analysis\n",
        "\n",
        "In this exercise, we’ll continue to study the Titanic dataset from the last exercise. Now that we have done some preprocessing, it’s time to look at the data with some exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5nTB9ExpYkM"
      },
      "source": [
        "1. First investigate each feature variable in turn. For each categorical variable, find out the mode, i.e., the most frequent value. For numerical variables, calculate the median value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hKdMpHZApYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PassengerId median 446.0\n",
            "Survived median 0.0\n",
            "Pclass median 3.0\n",
            "Sex median 1.0\n",
            "Age median 29.69911764705882\n",
            "SibSp median 0.0\n",
            "Parch median 0.0\n",
            "Embarked median 2.0\n",
            "Cabin_let median 5.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "with open(\"train2.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "res = {}\n",
        "\n",
        "for col in df.columns:\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        res[col] = (\"median\", df[col].median())\n",
        "    else:\n",
        "        res[col] = (\"mode\", df[col].mode().iloc[0])\n",
        "\n",
        "for col, (stat, value) in res.items():\n",
        "    print(col, stat, value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOjLqfDkpYkM"
      },
      "source": [
        "2. Next, combine the modes of the categorical variables, and the medians of the numerical variables, to construct an imaginary “average survivor”. This \"average survivor\" should represent the typical passenger of the class of passengers who survived. Also following the same principle, construct the “average non-survivor”.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 1: What are the average/most frequent variable values for a non-survivor?*</span>\n",
        "    \n",
        "    <span style=\"font-weight: 500\">*Hint 2: You can split the dataframe in two: one subset containing all the survivors and one consisting of all the non-survivor instances. Then, you can use the summary statistics of each of these dataframe to create a prototype \"average survivor\" and \"average non-survivor\", respectively.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OhUU2GIDpYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'PassengerId': ('median', 446.0), 'Pclass': ('median', 3.0), 'Sex': ('median', 1.0), 'Age': ('median', 29.69911764705882), 'SibSp': ('median', 0.0), 'Parch': ('median', 0.0), 'Embarked': ('median', 2.0), 'Cabin_let': ('median', 5.0)}\n",
            "{'PassengerId': ('median', 446.0), 'Pclass': ('median', 3.0), 'Sex': ('median', 1.0), 'Age': ('median', 29.69911764705882), 'SibSp': ('median', 0.0), 'Parch': ('median', 0.0), 'Embarked': ('median', 2.0), 'Cabin_let': ('median', 5.0)}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "with open(\"train2.json\", \"r\") as file:\n",
        "\tdata = json.load(file)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n",
        "def create_res(dataframe):\n",
        "\tres = {}    \n",
        "\tfor col in df.columns:\n",
        "\t\tif col == \"Survived\":\n",
        "\t\t\t\tcontinue\n",
        "\t\tif pd.api.types.is_numeric_dtype(df[col]):\n",
        "\t\t\tres[col] = (\"median\", df[col].median())\n",
        "\t\telse:\n",
        "\t\t\tres[col] = (\"mode\", df[col].mode().iloc[0])\n",
        "\treturn res\n",
        "\n",
        "df_surv = df[df[\"Survived\"] == 1]\n",
        "df_nonsurv = df[df[\"Survived\"] == 0]\n",
        "\n",
        "survivor = create_res(df_surv)\n",
        "nonsurv = create_res(df_nonsurv)\n",
        "print(survivor)\n",
        "print(nonsurv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Bax_hlpYkM"
      },
      "source": [
        "3. Next, let's study the distributions of the variables in the two groups (survivor/non-survivor). How well do the average cases represent the respective groups? Can you find actual passengers that are very similar to the (average) representative of their own group? Can you find passengers that are very similar to the (average) representative of the other group?\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Note: Feel free to choose EDA methods according to your preference: non-graphical/graphical, static/interactive - anything goes.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNsSMXRMpYkM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "with open(\"train2.json\", \"r\") as file:\n",
        "\tdata = json.load(file)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n",
        "def create_res(dataframe):\n",
        "\tres = {}    \n",
        "\tfor col in df.columns:\n",
        "\t\tif col == \"Survived\":\n",
        "\t\t\t\tcontinue\n",
        "\t\tif pd.api.types.is_numeric_dtype(df[col]):\n",
        "\t\t\tres[col] = (\"median\", df[col].median())\n",
        "\t\telse:\n",
        "\t\t\tres[col] = (\"mode\", df[col].mode().iloc[0])\n",
        "\treturn res\n",
        "\n",
        "df_surv = df[df[\"Survived\"] == 1]\n",
        "df_nonsurv = df[df[\"Survived\"] == 0]\n",
        "\n",
        "survivor = create_res(df_surv)\n",
        "nonsurv = create_res(df_nonsurv)\n",
        "\n",
        "\n",
        "\n",
        "def avg_row(dic):\n",
        "    avg = {}\n",
        "\t\n",
        "    for col, (stat, value) in dic.items():\n",
        "        avg_row[col] = value\n",
        "    return avg\n",
        "\n",
        "\n",
        "avg_surv = avg_row(survivor)\n",
        "avg_nonsurv = avg_row(nonsurv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc96MC9CpYkM"
      },
      "source": [
        "4. Next, let's continue the analysis by looking into pairwise and multivariate relationships between the variables in the two groups. Try to visualize two variables at a time using, e.g., scatter plots and use a different color to encode the survival status.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 1: You can also check out Seaborn's pairplot function, if you wish.*</span>\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 2: To better show many data points with the same value for a given variable, you can use either transparency or ‘jitter’.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fexBqTMQpYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNOrCjKzpYkM"
      },
      "source": [
        "5. Finally, recall the preprocessing we did in the first exercise. What can you say about the effect of the choices that were made to use the mode and mean to impute missing values, instead of, for example, ignoring passengers with missing data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8TeRhWapYkM"
      },
      "source": [
        "*This is a good way to preserve all available data, since the dataset is fairly small. It's a good way to widen the amount of data since nothing is lost. If we were to drop the values entirely, the dataset could become unreliable due to biased results. It does reduce the variability though, since the filled in data is already present in the set. This means that some outliers can get less noticable since the entire set is focused more on the average point.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrhB5wvpYkM"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqgXjyklpYkM"
      },
      "source": [
        "## Exercise 3 | Working with text data 2.0\n",
        "\n",
        "This exercise is related to the second exercise from last week. Find the saved <span style=\"font-weight: 500\">pos.txt</span> and <span style=\"font-weight: 500\">neg.txt</span> files, or, alternatively, you can find the week 1 example solutions on the MOOC platform after Tuesday."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-DfVlSWpYkM"
      },
      "source": [
        "1. Find the most common words in each file (positive and negative). Examine the results. Do they tend to be general terms relating to the nature of the data? How well do they indicate positive/negative sentiment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDsK6jXMpYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos [('great', 435906), ('work', 427780), ('use', 345233), ('good', 291448), ('fit', 273505), ('instal', 229408), ('product', 210084), ('look', 181401), ('just', 180420), ('like', 175725)]\n",
            "neg [('work', 39998), ('use', 37244), ('fit', 31293), ('product', 24394), ('just', 23580), ('light', 22907), ('like', 20685), ('time', 19665), ('look', 18824), ('instal', 18709)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "with open(\"pos.txt\", \"r\", encoding=\"utf-8\") as pos_file:\n",
        "    pos_data = pos_file.read()\n",
        "\n",
        "split_it = pos_data.split()\n",
        "Counters_found = Counter(split_it)\n",
        "\n",
        "most_occur = Counters_found.most_common(10)\n",
        "print(f\"pos {most_occur}\")\n",
        "\n",
        "\n",
        "\n",
        "with open(\"neg.txt\", \"r\", encoding=\"utf-8\") as neg_file:\n",
        "    neg_data = neg_file.read()\n",
        "\n",
        "split_it = neg_data.split()\n",
        "Counters_found = Counter(split_it)\n",
        "\n",
        "most_occur = Counters_found.most_common(10)\n",
        "print(f\"neg {most_occur}\")\n",
        "\n",
        "#the words do seem to be more general terms and its not really possible to analyse the sentiment behind them from just the most common words\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlU_trsrpYkM"
      },
      "source": [
        "2. Compute a [TF/IDF](https://en.wikipedia.org/wiki/Tf–idf) vector for each of the two text files, and make them into a <span style=\"font-weight: 500\">2 x m</span> matrix, where <span style=\"font-weight: 500\">m</span> is the number of unique words in the data. The problem with using the most common words in a review to analyze its contents is that words that are common overall will be common in all reviews (both positive and negative). This means that they probably are not good indicators about the sentiment of a specific review. TF/IDF stands for Term Frequency / Inverse Document Frequency (here the reviews are the documents), and is designed to help by taking into consideration not just the number of times a term occurs (term frequency), but also how many times a word exists in other reviews as well (inverse document frequency). You can use any variant of the formula, as well as off-the-shelf implementations. <span style=\"font-weight: 500\">*Hint: You can use [sklearn](http://scikit-learn.org/).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Tt_t-Lx8pYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 235581)\n",
            "           00       000      0000     00000    000000  00000000  \\\n",
            "Pos  0.000372  0.000072  0.000045  0.000003  0.000002  0.000000   \n",
            "Neg  0.000219  0.000174  0.000060  0.000008  0.000008  0.000011   \n",
            "\n",
            "     000000000000000000058  000000000000002  00000000004   0000001  ...  \\\n",
            "Pos               0.000001         0.000001     0.000004  0.000002  ...   \n",
            "Neg               0.000000         0.000000     0.000000  0.000000  ...   \n",
            "\n",
            "       zzzzip         zzzzz  zzzzzooooo        zzzzzz   zzzzzzt   zzzzzzz  \\\n",
            "Pos  0.000001  8.414345e-07    0.000001  8.414345e-07  0.000000  0.000000   \n",
            "Neg  0.000000  1.509088e-05    0.000000  7.545442e-06  0.000011  0.000011   \n",
            "\n",
            "     zzzzzzzz  zzzzzzzzip  zzzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzz  \n",
            "Pos  0.000001    0.000001        0.000001              0.000002  \n",
            "Neg  0.000000    0.000000        0.000000              0.000000  \n",
            "\n",
            "[2 rows x 235581 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "with open(\"pos.txt\", \"r\", encoding=\"utf-8\") as pos_file:\n",
        "    pos_data = pos_file.read()\n",
        "\n",
        "with open(\"neg.txt\", \"r\", encoding=\"utf-8\") as neg_file:\n",
        "    neg_data = neg_file.read()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([pos_data, neg_data])\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "words = vectorizer.get_feature_names()\n",
        "\n",
        "df = pd.DataFrame(tfidf_array, index=[\"Pos\", \"Neg\"], columns=words)\n",
        "print(df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lsOuLeOpYkM"
      },
      "source": [
        "3. List the words with the highest TF/IDF score in each class (positive | negative), and compare them to the most common words. What do you notice? Did TF/IDF work as expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1tkcbH5pYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 235581)\n",
            "           00       000      0000     00000    000000  00000000  \\\n",
            "Pos  0.000372  0.000072  0.000045  0.000003  0.000002  0.000000   \n",
            "Neg  0.000219  0.000174  0.000060  0.000008  0.000008  0.000011   \n",
            "\n",
            "     000000000000000000058  000000000000002  00000000004   0000001  ...  \\\n",
            "Pos               0.000001         0.000001     0.000004  0.000002  ...   \n",
            "Neg               0.000000         0.000000     0.000000  0.000000  ...   \n",
            "\n",
            "       zzzzip         zzzzz  zzzzzooooo        zzzzzz   zzzzzzt   zzzzzzz  \\\n",
            "Pos  0.000001  8.414345e-07    0.000001  8.414345e-07  0.000000  0.000000   \n",
            "Neg  0.000000  1.509088e-05    0.000000  7.545442e-06  0.000011  0.000011   \n",
            "\n",
            "     zzzzzzzz  zzzzzzzzip  zzzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzz  \n",
            "Pos  0.000001    0.000001        0.000001              0.000002  \n",
            "Neg  0.000000    0.000000        0.000000              0.000000  \n",
            "\n",
            "[2 rows x 235581 columns]\n",
            "great      0.366786\n",
            "work       0.359949\n",
            "use        0.290491\n",
            "good       0.245234\n",
            "fit        0.230137\n",
            "instal     0.193032\n",
            "product    0.176772\n",
            "look       0.152637\n",
            "just       0.151812\n",
            "like       0.147861\n",
            "Name: Pos, dtype: float64\n",
            "work       0.301803\n",
            "use        0.281022\n",
            "fit        0.236120\n",
            "product    0.184064\n",
            "just       0.177922\n",
            "light      0.172843\n",
            "like       0.156077\n",
            "time       0.148381\n",
            "look       0.142035\n",
            "instal     0.141168\n",
            "Name: Neg, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "with open(\"pos.txt\", \"r\", encoding=\"utf-8\") as pos_file:\n",
        "    pos_data = pos_file.read()\n",
        "\n",
        "with open(\"neg.txt\", \"r\", encoding=\"utf-8\") as neg_file:\n",
        "    neg_data = neg_file.read()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([pos_data, neg_data])\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "words = vectorizer.get_feature_names()\n",
        "\n",
        "df = pd.DataFrame(tfidf_array, index=[\"Pos\", \"Neg\"], columns=words)\n",
        "print(df.head(10))\n",
        "\n",
        "top_pos = df.loc[\"Pos\"].sort_values(ascending=False).head(10)\n",
        "top_neg = df.loc[\"Neg\"].sort_values(ascending=False).head(10)\n",
        "\n",
        "print(top_pos)\n",
        "print(top_neg)\n",
        "\n",
        "\n",
        "#i notice that they match quite well. I thinki it did work as expected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z_vWhnXpYkM"
      },
      "source": [
        "4. Plot the words in each class with their corresponding TF/IDF scores. Note that there will be a lot of words, so you’ll have to think carefully to make your chart clear! If you can’t plot them all, plot a subset – think about how you should choose this subset.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint: you can use word clouds. But feel free to challenge yourselves to think of any other meaningful way to visualize this information!*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eUJ3HxlpYkN"
      },
      "outputs": [],
      "source": [
        "#tried to do this but had no idea on how to proceed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8-yzV5HpYkN"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Xd1n98pYkQ"
      },
      "source": [
        "## Exercise 4 | Junk charts\n",
        "\n",
        "There’s a thriving community of chart enthusiasts who keep looking for statistical graphics that they find inappropriate, and which they call “junk charts”, and who often also propose ways to improve them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w78AogNWpYkQ"
      },
      "source": [
        "1. Find at least three statistical visualizations you think are not very good and identify their problems. Copying examples from various junk chart websites is not accepted – you should find your own junk charts, out in the wild. You should be able to find good (or rather, bad) examples quite easily since a significant fraction of charts can have at least *some* issues. The examples you choose should also have different problems, e.g., try to avoid collecting three bar charts, all with problematic axes. Instead, try to find as interesting and diverse examples as you can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJfbaQH2pYkQ"
      },
      "source": [
        "2. Try to produce improved versions of the charts you selected. The data is of course often not available, but perhaps you can try to extract it, at least approximately, from the chart. Or perhaps you can simulate data that looks similar enough to make the point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-rwa7IrpYkQ"
      },
      "source": [
        "**Submit a PDF with all the charts (the ones you found and the ones you produced).**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
